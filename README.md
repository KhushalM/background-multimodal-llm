# ğŸ¤ğŸ–¥ï¸ Multimodal AI Assistant

A real-time multimodal AI assistant that combines voice interaction with screen context awareness. Talk to AI while it can see and understand what's on your screen.

## âœ¨ Features

### ğŸ¯ Core Capabilities

- **ğŸ¤ Real-time Voice Chat** - Continuous conversation with VAD (Voice Activity Detection)
- **ğŸ–¥ï¸ Screen Context Awareness** - AI can see and analyze your screen when relevant
- **ğŸ§  Smart Screen Triggers** - Automatically captures screen based on conversation context
- **âš¡ Fast Response Times** - Optimized for real-time interaction
- **ğŸ”„ WebSocket Communication** - Low-latency bidirectional communication

### ğŸ¤– AI Models & Services

- **Speech-to-Text**: OpenAI Whisper API
- **Text-to-Speech**: OpenAI TTS API
- **Multimodal AI**: Google Gemini 2.0 Flash Exp
- **Voice Activity Detection**: Silero VAD (via @ricky0123/vad-react)

### ğŸ› ï¸ Technical Stack

- **Frontend**: React 18 + TypeScript + Vite + Chakra UI
- **Backend**: FastAPI + Python 3.11 + WebSockets
- **Deployment**: Docker + AWS EC2 + GitHub Actions CI/CD
- **Development**: Dev Containers + Hot Reload

## ğŸš€ Quick Start

### ğŸ³ Development with Dev Container (Recommended)

```bash
# 1. Open in VS Code with Dev Container extension
# 2. Container auto-setups dependencies
# 3. Start services:

# Terminal 1 - Backend
cd backend && python main.py

# Terminal 2 - Frontend
cd frontend && npm run dev
```

### ğŸ’» Manual Local Development

```bash
# Backend setup
cd backend
uv pip install -r requirements.txt
cp env.example .env  # Configure your API keys
python main.py

# Frontend setup (new terminal)
cd frontend
npm install
npm run dev
```

### ğŸŒ Access URLs

- **Frontend**: http://localhost:3000
- **Backend**: http://localhost:8000
- **Health Check**: http://localhost:8000/health

## âš™ï¸ Configuration

### ğŸ”‘ Required API Keys

Copy `backend/env.example` to `backend/.env`:

```bash
# Required for full functionality
OPENAI_API_KEY=your_openai_api_key_here
GEMINI_API_KEY=your_gemini_api_key_here

# Optional
HUGGINGFACE_API_TOKEN=your_huggingface_token_here
SECRET_KEY=your_secret_key_here
```

### ğŸ¤ Browser Permissions

**Important**: For voice features to work:

- Use **HTTPS** in production (required for microphone access)
- For local development: Chrome/Firefox will ask for microphone permission
- Allow microphone access when prompted

## ğŸ—ï¸ Project Structure

```
â”œâ”€â”€ backend/              # FastAPI backend
â”‚   â”œâ”€â”€ main.py          # Main application entry point
â”‚   â”œâ”€â”€ models/          # AI model integrations
â”‚   â”œâ”€â”€ services/        # Core services & managers
â”‚   â”œâ”€â”€ env.example      # Environment configuration template
â”‚   â””â”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ frontend/            # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/  # React components
â”‚   â”‚   â”œâ”€â”€ hooks/       # Custom React hooks
â”‚   â”‚   â””â”€â”€ services/    # API service layers
â”‚   â””â”€â”€ package.json     # Node dependencies
â”œâ”€â”€ deployment/          # All deployment files
â”‚   â”œâ”€â”€ docker-compose.dev.yml     # Development deployment
â”‚   â”œâ”€â”€ scripts/setup-aws-dev.sh   # AWS infrastructure setup
â”‚   â””â”€â”€ infrastructure/            # CloudFormation templates
â”œâ”€â”€ docs/               # Documentation
â””â”€â”€ .github/workflows/  # CI/CD pipelines
```

## ğŸ”§ Development

### ğŸ› ï¸ Available Commands

```bash
# Backend
cd backend
python main.py              # Start development server
uv pip install -r requirements.txt  # Install dependencies

# Frontend
cd frontend
npm run dev                  # Start development server
npm run build               # Build for production
npm run preview             # Preview production build

# Deployment
docker-compose -f deployment/docker-compose.dev.yml up -d  # Start with Docker
```

### ğŸ” Debugging

```bash
# View logs
docker-compose -f deployment/docker-compose.dev.yml logs -f

# Check specific service
docker-compose -f deployment/docker-compose.dev.yml logs backend
docker-compose -f deployment/docker-compose.dev.yml logs frontend

# Restart services
docker-compose -f deployment/docker-compose.dev.yml restart
```

## ğŸš€ Deployment

### â˜ï¸ AWS Production Deployment

```bash
# 1. Setup AWS infrastructure
chmod +x deployment/scripts/setup-aws-dev.sh
./deployment/scripts/setup-aws-dev.sh

# 2. Configure GitHub Secrets (for CI/CD)
# Go to: https://github.com/your-repo/settings/secrets/actions
# Add these secrets:
# - DEV_EC2_INSTANCE_IP: Your EC2 public IP
# - DEV_EC2_SSH_PRIVATE_KEY: Your EC2 private key content
# - OPENAI_API_KEY: Your OpenAI API key
# - GEMINI_API_KEY: Your Gemini API key

# 3. Deploy via GitHub Actions
git push origin main  # Triggers automatic deployment
```

### ğŸ”„ Continuous Deployment

- **Push to `main`** â†’ Automatic production deployment
- **Pull requests** â†’ Automatic testing
- **Health checks** â†’ Automatic validation
- **Rollback support** â†’ Safe deployments

### ğŸ’° AWS Costs

- **Development**: ~$5-15/month (free tier eligible)
- **Production**: ~$25-50/month (depends on usage)

## ğŸ¯ Usage Guide

### ğŸ¤ Voice Interaction

1. **Click "Start Voice Assistant"**
2. **Grant microphone permission** when prompted
3. **Start talking** - VAD automatically detects speech
4. **AI responds** with voice and text

### ğŸ–¥ï¸ Screen Context

- **Smart Triggers**: AI automatically captures screen when you say things like:
  - "Can you see my screen?"
  - "What's this error?"
  - "Help me with this"
- **Manual Capture**: Click "Share Screen" for continuous sharing
- **Privacy**: Screen capture only when explicitly needed

### ğŸ’¡ Pro Tips

- **Clear Speech**: Speak clearly for better transcription
- **Context Clues**: Use phrases like "look at this" to trigger screen capture
- **Error Debugging**: Say "what's wrong here?" while viewing errors
- **Natural Conversation**: Talk naturally - the AI understands context

## ğŸ”§ Advanced Configuration

### ğŸ›ï¸ VAD Sensitivity

Adjust in `frontend/src/hooks/useVoiceAgent.ts`:

```typescript
const vadOptions = {
  positiveSpeechThreshold: 0.8, // Higher = less sensitive
  negativeSpeechThreshold: 0.2, // Lower = less sensitive
  minSpeechFrames: 3, // Minimum frames for speech detection
};
```

### ğŸ–¥ï¸ Screen Capture Settings

Configure in `backend/main.py`:

```python
SCREEN_TRIGGER_CONFIDENCE = 0.7  # Confidence threshold for auto-capture
SCREEN_CAPTURE_QUALITY = 0.8     # Image quality (0.1-1.0)
```

## ğŸ› Troubleshooting

### Common Issues

**ğŸ¤ Microphone not working**

- Ensure HTTPS (required in production)
- Check browser permissions
- Try refreshing the page

**ğŸ–¥ï¸ Screen sharing not working**

- Use Chrome/Firefox (Safari has limitations)
- Grant screen sharing permission
- Check for browser extensions blocking

**âš¡ Slow responses**

- Check your internet connection
- Verify API keys are configured
- Monitor backend logs for errors

**ğŸ”Œ Connection issues**

- Check WebSocket connection in browser dev tools
- Verify backend is running on port 8000
- Check firewall settings

### ğŸ“Š Health Monitoring

- **Health Check**: http://localhost:8000/health
- **Performance**: http://localhost:8000/performance
- **Logs**: `docker-compose logs -f`

## ğŸ¤ Contributing

1. **Fork the repository**
2. **Create feature branch**: `git checkout -b feature/amazing-feature`
3. **Commit changes**: `git commit -m 'Add amazing feature'`
4. **Push to branch**: `git push origin feature/amazing-feature`
5. **Open Pull Request**

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ†˜ Support

- **ğŸ› Bug Reports**: [GitHub Issues](https://github.com/your-repo/issues)
- **ğŸ’¡ Feature Requests**: [GitHub Discussions](https://github.com/your-repo/discussions)
- **ğŸ“– Documentation**: [./docs/](./docs/)

---

**Built with â¤ï¸ for seamless human-AI interaction**
